{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def attention(query, key, value, mask):\n",
    "    d_k = query.shape[-1]\n",
    "    # Just apply the formula from the paper\n",
    "    # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "    attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "        attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "    attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "    return (attention_scores @ value)\n",
    "\n",
    "def dilated_attention_singlehead(x, w_seq, r_seq):\n",
    "    seq_length = x.shape[2]\n",
    "    k = len(w_seq) # The number of combinations of segment lengths (w) and dilations (r)\n",
    "\n",
    "    all_O = []\n",
    "    all_s_i = []\n",
    "\n",
    "    for param_index in range(k):\n",
    "        w = w_seq[param_index]\n",
    "        r = r_seq[param_index]\n",
    "\n",
    "        all_O_hats = []\n",
    "\n",
    "        for i in range(seq_length // w):\n",
    "            segment_content = x[:, i * w : (i + 1) * w, :]\n",
    "\n",
    "            # Apply the dilation, by zeroing out every rth column\n",
    "            mask = torch.zeros_like(segment_content)\n",
    "            mask[:, ::r, :] = 1  \n",
    "            segment_content = segment_content * mask\n",
    "\n",
    "            query_tilde = segment_content\n",
    "            key_tilde = segment_content\n",
    "            value_tilde = segment_content\n",
    "            \n",
    "            O_tilde = query_tilde @ key_tilde.transpose(-1, -2)\n",
    "            # Zero out out everything above the diagonal to make it causal\n",
    "            mask = torch.tril(torch.ones_like(O_tilde), diagonal=0)\n",
    "            O_tilde = O_tilde * mask\n",
    "            O_tilde = torch.softmax(O_tilde, dim=-1)\n",
    "            O_tilde = O_tilde @ value_tilde\n",
    "\n",
    "            # Make all the columns for which column_index % r != 0 equal to 0\n",
    "            # This is the same as the mask used in the paper\n",
    "            mask = torch.zeros_like(O_tilde)\n",
    "            mask[:, :, ::r] = 1\n",
    "            o_hat = O_tilde * mask\n",
    "\n",
    "            all_O_hats.append(o_hat)\n",
    "\n",
    "\n",
    "        O = torch.cat(all_O_hats, dim=1)\n",
    "        # According to the paper: \"s_i\" is the denominator of the attention softmax for O\n",
    "        s_i = torch.sum(torch.exp(O)).item()\n",
    "        all_O.append(O)\n",
    "        all_s_i.append(s_i)\n",
    "\n",
    "    sum_s_i = sum(all_s_i)\n",
    "    alpha_i = [s_i / sum_s_i for s_i in all_s_i]\n",
    "    sum_terms = [alpha_i[i] * all_O[i] for i in range(len(all_O))]\n",
    "    # Sum all the tensors in sum_terms\n",
    "    return sum(sum_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for normal attention: 0.27 seconds\n",
      "Time taken for dilated attention: 19.45 seconds\n"
     ]
    }
   ],
   "source": [
    "#Shape: (batch_size, num_heads, seq_len, dim)\n",
    "\n",
    "\n",
    "# embedding size\n",
    "d_model = 1024\n",
    "\n",
    "w_seq = [4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "r_seq = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "\n",
    "# for test_index in range(len(seq_multipliers)):\n",
    "seq_len = 1024\n",
    "x = torch.rand(32, 1, seq_len, d_model).to(torch.float)\n",
    "\n",
    "start_time = time.time()\n",
    "# calculate the attention according to \"Attention is all you need\"\n",
    "causal_mask = torch.tril(torch.ones(seq_len, seq_len), diagonal=0)\n",
    "attn_normal = attention(x, x, x, causal_mask)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken for normal attention: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "# calculate the dilated attention\n",
    "attn_dilated = dilated_attention_singlehead(x, w_seq, r_seq)\n",
    "assert attn_dilated.shape == x.shape\n",
    "end_time = time.time()\n",
    "print(f\"Time taken for dilated attention: {end_time - start_time:.2f} seconds\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
