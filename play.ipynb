{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from attend import Attend\n",
    "\n",
    "# Shape: (batch_size, num_heads, seq_len, dim)\n",
    "# x = torch.randn(32, 1, 72, 512)\n",
    "\n",
    "# attn = Attend(flash=True)\n",
    "\n",
    "# # calculate the attention\n",
    "# scores = attn.forward(x, x, x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0, 1000, (32, 16, 512)).to(torch.float) # Input to the attention\n",
    "\n",
    "seq_length = x.shape[1]\n",
    "\n",
    "# The alpha constant for the geometric series is 2\n",
    "# w_seq = [4, 8, 16]\n",
    "# r_seq = [1, 2, 4]\n",
    "\n",
    "w_seq = [8]\n",
    "r_seq = [2]\n",
    "\n",
    "k = len(w_seq) # The number of combinations of segment lengths (w) and dilations (r)\n",
    "num_heads = 4\n",
    "\n",
    "for param_index in range(k):\n",
    "    w = w_seq[param_index]\n",
    "    r = r_seq[param_index]\n",
    "\n",
    "    for i in range(seq_length // w):\n",
    "        for j in range(num_heads):\n",
    "            s_j = j % r\n",
    "            segment_indices = torch.arange(i * w + s_j, (i + 1) * w, r)\n",
    "\n",
    "            print (f'parameters={param_index}, segment_index={i}, head={j}')\n",
    "            print(segment_indices)\n",
    "\n",
    "            query_tilde = x[:, segment_indices, :]\n",
    "            key_tilde = x[:, segment_indices, :]\n",
    "            value_tilde = x[:, segment_indices, :]\n",
    "            \n",
    "            O_tilde = query_tilde @ key_tilde.transpose(-1, -2) \n",
    "            O_tilde = torch.softmax(O_tilde, dim=-1)\n",
    "            O_tilde = O_tilde @ value_tilde\n",
    "\n",
    "            # Make all the columns for which column_index % r != 0 equal to 0\n",
    "            # This is the same as the mask used in the paper\n",
    "            mask = torch.zeros_like(O_tilde)\n",
    "            mask[:, :, ::r] = 1\n",
    "            o_hat = O_tilde * mask\n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # print ((O_i).shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters=0, segment_index=0, head=0\n",
      "tensor([0, 2, 4, 6])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m     mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(O_tilde)\n\u001b[1;32m     38\u001b[0m     mask[:, :, ::r] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 39\u001b[0m     o_hat \u001b[39m=\u001b[39m O_tilde \u001b[39m*\u001b[39m mask\n\u001b[1;32m     46\u001b[0m \u001b[39m# print ((O_i).shape)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[39], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m     mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(O_tilde)\n\u001b[1;32m     38\u001b[0m     mask[:, :, ::r] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 39\u001b[0m     o_hat \u001b[39m=\u001b[39m O_tilde \u001b[39m*\u001b[39m mask\n\u001b[1;32m     46\u001b[0m \u001b[39m# print ((O_i).shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/longnet/lib/python3.11/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:988\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[39m# if thread has a suspend flag, we suspend with a busy wait\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[39mif\u001b[39;00m info\u001b[39m.\u001b[39mpydev_state \u001b[39m==\u001b[39m STATE_SUSPEND:\n\u001b[0;32m--> 988\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_wait_suspend(thread, frame, event, arg)\n\u001b[1;32m    989\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrace_dispatch\n\u001b[1;32m    990\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/longnet/lib/python3.11/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:165\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdo_wait_suspend\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 165\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_args[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mdo_wait_suspend(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/longnet/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/longnet/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.01\u001b[39m)\n\u001b[1;32m   2108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x = torch.randint(0, 1000, (32, 16, 512)).to(torch.float) # Input to the attention\n",
    "\n",
    "seq_length = x.shape[1]\n",
    "\n",
    "# The alpha constant for the geometric series is 2\n",
    "# w_seq = [4, 8, 16]\n",
    "# r_seq = [1, 2, 4]\n",
    "\n",
    "w_seq = [8]\n",
    "r_seq = [2]\n",
    "\n",
    "k = len(w_seq) # The number of combinations of segment lengths (w) and dilations (r)\n",
    "num_heads = 4\n",
    "\n",
    "for param_index in range(k):\n",
    "    w = w_seq[param_index]\n",
    "    r = r_seq[param_index]\n",
    "\n",
    "    for i in range(seq_length // w):\n",
    "        for j in range(num_heads):\n",
    "            s_j = j % r\n",
    "            segment_indices = torch.arange(i * w + s_j, (i + 1) * w, r)\n",
    "\n",
    "            print (f'parameters={param_index}, segment_index={i}, head={j}')\n",
    "            print(segment_indices)\n",
    "\n",
    "            query_tilde = x[:, segment_indices, :]\n",
    "            key_tilde = x[:, segment_indices, :]\n",
    "            value_tilde = x[:, segment_indices, :]\n",
    "            \n",
    "            O_tilde = query_tilde @ key_tilde.transpose(-1, -2) \n",
    "            O_tilde = torch.softmax(O_tilde, dim=-1)\n",
    "            O_tilde = O_tilde @ value_tilde\n",
    "\n",
    "            # Make all the columns for which column_index % r != 0 equal to 0\n",
    "            # This is the same as the mask used in the paper\n",
    "            mask = torch.zeros_like(O_tilde)\n",
    "            mask[:, :, ::r] = 1\n",
    "            o_hat = O_tilde * mask\n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # print ((O_i).shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
